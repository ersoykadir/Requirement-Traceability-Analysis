\section{Discussion}
\label{sec:discuss}

\emph{Observations.} Our experiments on the evaluation of the tool have led us to observe that the quality and consistency of the requirement statements and the software development artifacts (SDA) significantly impact the effectiveness of our approach in identifying trace links. Well-written requirement specifications that are self-explanatory and granulated lead to better performance of its traces along with software development artifacts that have textual data (e.g. title, description, comments) that is semantically related to the feature they implement.

\emph{Limitations.} The current implementation of our approach traces requirements to issues, pull requests, and commits. A more thorough trace should include other artifacts such as the architecture models, code, and test cases. Our implementation handles English only, other languages are not supported. The keyword matching method focuses on the syntax rather than the semantics of the word, yet consistent use of terms and agreeing on a glossary should mitigate this limitation.

\emph{Implications in practice.} Our approach aims to visualize software repositories based on trace links. Improving the performance of trace link extraction would provide more accurate data for the dashboard visualization. We hypothesize that software engineering educators and students can benefit from this visualization to track the progress of the software projects both during and after the development phase. Visualization of traces of a requirement presents the maturity of the implementation of a requirement and the contribution patterns of students. Similarly, companies can track the progress of their projects and can gain retrospective insights on projects for good and bad practices based on the visualizations provided in our dashboard.

\emph{Threats to validity.} Our evaluation presented in this paper is preliminary. To mitigate the threats to internal validity, we selected a repository the authors were not involved and two authors collaborated on building the ground truth to reduce personal bias. We cannot reach any conclusions about the external validity of the results. We need to conduct more studies to generalize the results. We share our replication package and invite the community to replicate our work for reliability.