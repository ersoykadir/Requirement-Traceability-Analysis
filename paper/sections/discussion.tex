\section{Discussion}
\label{sec:discuss}

\emph{Observations.} Our experiments on the evaluation of the tool have led us to observe that the quality and consistency of the requirement statements and the software development artifacts (SDA) significantly impact the effectiveness of our approach in identifying trace links. Well-written requirement specifications that are self-explanatory and granulated lead to better performance of its traces along with software development artifacts which have textual data (e.g. title, description, comments) that is semantically related to the feature they implement.

\emph{Limitations.} The current implementation of our approach traces requirements to issues, pull requests, and commits. A more thorough trace should include other artifacts such as the architecture models, code, and test cases. Our implementation handles English only, other languages are not supported. Keyword matching method focuses on the syntax rather than the semantics of the word, yet consisted use of terms and agreeing on a glossary should mitigate this limitation.

\emph{Implications in practice.} Our approach aims to visualize software repositories based on the trace links. Improving the performance of trace link extraction would provide more accurate data for the dashboard visualization. We hypothesize that software engineering educators and students can benefit from these visualization to track the progress of the software projects both during and after the development phase. Visualizaiton of traces of a requirement presents the maturity of implementation of a requirement and the contribution patterns of students. Similarly, companies can track progress of their projects and can gain retrospective insights on projects for good and bad practices based on the visualizations provided in our dashboard.

\emph{Threats to validity.} Our evaluation presented in this paper is preliminary. To mitigate the threats to internal validity, we selected a repository the authors were not involved and two authors collaborated on building the ground truth to reduce personal bias. We cannot reach any conclusions for the external validity of the results. We need to conduct more studies to generalize the results. We share our replication package and invite the community to replicate our work for reliability.





%It is important to acknowledge the threats to the validity of our study. Firstly, the absence of a ground truth set that is established by analysts may introduce bias in our evaluation process. We have prepared the ground truth set and conducted our experiments on the student projects we have participated in. Thus, we do know the capabilities of our tool and the project that we experimented on. Future work should involve validation with industry professionals and experiments designed with a wider range of projects to enhance the operating scale of our tool.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: