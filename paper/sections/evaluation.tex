\section{Evaluation}
\label{sec:eval}

This section presents the results of our preliminary evaluation and detailed evaluation plan for the future.

We evaluate our approach on a public GitHub repository of a group of computer engineering students for their software engineering course\footnote{Link Anonymized.}. The repository includes the project of an online learning platform. The requirements of the project are also in the repository written in a mixed format where some requirements are written as short phrases to describe a functionality whereas others as full shall statements. The requirements are structured hierarchically where a parent requirement is further refined into other requirement items.

The first two authors who are familiar with the project but not involved in the project manually extracted the trace links to serve as the ground truth. Below we report the performance of our keyword matching method in Table~\ref{tab:keyperf} and vector-based methods with various similarity thresholds in Table~\ref{tab:vecperf}.

\begin{table}[htb]
\centering
\caption{\label{tab:keyperf}Performance of the Keyword Matching Method}
\begin{tabular}{llll}
  \toprule
  Method & Recall & Precision & F1 Score \\ \midrule
  Keyword extraction & {0.865} & 0.212 & 0.340 \\
  \bottomrule
\end{tabular}
\end{table}

\begin{table}[htb]
\centering
\caption{Performance of the Vector-based Methods}
\label{tab:vecperf}
\begin{tabular}{lllllll}
   \toprule
    \multirow{2}{*}{\shortstack[l]{Similarity \\ Threshold}}
  & \multicolumn{3}{c}{Word-vector} &  \multicolumn{3}{c}{TF-IDF vector} \\
   \cmidrule{2-7}
                                                          & {Rec.} & {Prec.} & F1 &  {Rec.} & {Prec.} & F1\\
   \midrule
  0.05 & 1 & 0.043 & 0.082 & 0.839 & 0.121 & 0.211 \\
  0.15 & 1 & 0.043 & 0.082 & 0.573 & 0.256 & 0.354 \\
  0.25 & 1 & 0.043 & 0.082 & 0.244 & 0.43 & 0.311 \\
  0.35 & 1 & 0.043 & 0.082 & 0.095 & 0.392 & 0.153 \\
  0.45 & 0.965 & 0.071 & 0.132 & 0.025 & 0.125 & 0.042 \\
  0.55 & 0.865 & 0.1 & 0.179 & 0.013 & 0.121 & 0.023 \\
  0.65 & 0.294 & 0.3 & 0.297 & 0 & 0 & -- \\
   \bottomrule
 \end{tabular}
\end{table}

Based on the F1 scores, the TF-IDF vector-based method has the best F1 score followed by the keyword-matching method on this repository followed by keyword matching method. The performance of the vector-based methods  significantly varies according to the threshold value. Setting a low threshold links requirements with many artifacts yet few of these links are actually valid.

We do not reach any conclusions based on our preliminary evaluation. This evaluation demonstrates that our approach is applicable for tracing requirements in a software repository but we refrain to be conclusive on the performance of the methods. In the near future, we plan two additional evaluation studies.

The first study concerns evaluating our approach again in an educational setting where we analyze the repositories of student groups, report on the perceived usefulness and usability of our dashboard, and study any correlations between the statistics reported by our dashboard and the performance of the groups in the course.

The second study is a case study with an industry partner where we ask for the ground truth from the experts from the industry and report the performance of different methods to extract trace links as well as the perceived usefulness and usability of our dashboard in comparison to trace matrix, which is widely used in the industry for tracing requirements.
