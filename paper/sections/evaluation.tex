\section{Evaluation}
\label{sec:eval}

% \begin{itemize}
%     \item Ground truth
%     \item Experiment setup
%     \item Results
%     \item Evaluation of results
% \end{itemize}

% This section describes how we evaluated our approach, which focuses on the accuracy of the method utilized in extracting the trace links, namely keyword extraction, TF-IDF vectors and word vectors.

This section presents the results of our preliminary evaluation and detailed evaluation plan for the future.

We evaluate our approach on a public Github repository of a group of computer engineering students for their software engineering course\footnote{Link Anonymized.%https://github.com/bounswe/bounswe2022group2
}. The repository includes the project of an online learning platform. The requirements of the project is also in the repository written in a mixed format where some requirements are written as short phrases to describe a functionality where others as full shall statements. The requirements are structured hierarchically where a parent requirement is further refined into other requirement items.

The first two authors who are familiar with the project but not involved in the project manually extracted the trace links to serve as the ground truth. Below we report the performance of our keyword matching method in Table~\ref{tab:keyperf} and vector-based methods with various similarity thresholds in Table~\ref{tab:vecperf}.

%For this purpose, we selected a set of requirements and thir associated software development artifactsto serve as a ground truth.
%The trace links were manually traced  by our team. \todo{Have we put the selected requirements and traces somewhere? If not let's put it in our repo. }

%The performance of our approach is evaluated using the recall and precision metrics, which inform us about the  percentage of the  traces that were successfully recovered and  the percentage of the traces that are correctly recovered using the following formulas:


%$Recall = \dfrac{True Positives}{True Positives + False Negatives}$


%$Precision = \dfrac{True Positives}{True Positives + False Positives}$


%In the experiment, true positives indicate the trace links that are identified by the method.
%False negatives indicate the trace links that are not identified(missed), and false positives indicate the identified trace %links that are incorrect.
%Table~\ref{tab:keyperf} presents the results of the keyword extraction method and
%Table~\ref{tab:vecperf} shows the results of the vector based methods.


\begin{table}[htb]
\centering
\caption{\label{tab:keyperf}Performance of the Keyword Matching Method}
\begin{tabular}{llll}
  \toprule
  Method & Recall & Precision & F1 Score \\ \midrule
  Keyword extraction & {0.865} & 0.212 & 0.340 \\
  \bottomrule
\end{tabular}
\end{table}

\begin{table}[htb]
\centering
\caption{Performance of the Vector-based Methods}
\label{tab:vecperf}
\begin{tabular}{lllllll}
   \toprule
    \multirow{2}{*}{\shortstack[l]{Similarity \\ Threshold}}
  & \multicolumn{3}{c}{Word-vector} &  \multicolumn{3}{c}{TF-IDF vector} \\
   \cmidrule{2-7}
                                                          & {Rec.} & {Prec.} & F1 &  {Rec.} & {Prec.} & F1\\
   \midrule
  0.05 & 1 & 0.043 & 0.082 & 0.839 & 0.121 & 0.211 \\
  0.15 & 1 & 0.043 & 0.082 & 0.573 & 0.256 & 0.354 \\
  0.25 & 1 & 0.043 & 0.082 & 0.244 & 0.43 & 0.311 \\
  0.35 & 1 & 0.043 & 0.082 & 0.095 & 0.392 & 0.153 \\
  0.45 & 0.965 & 0.071 & 0.132 & 0.025 & 0.125 & 0.042 \\
  0.55 & 0.865 & 0.1 & 0.179 & 0.013 & 0.121 & 0.023 \\
  0.65 & 0.294 & 0.3 & 0.297 & 0 & 0 & -- \\
   \bottomrule
 \end{tabular}
\end{table}

Based on the F1-scores, TF-IDF vector based method has the best F1 score followed by the keyword matching method on this repository followed by keyword matching method. The performance of the vector-based methods  significantly varies according to the threshold value. Setting a low thresholds links requirements with many artifacts yet few of these links are actually valid.


%The recall values  approach 1 with low  threshold values which also yield significantly low precision values.
% This indicates that the desired traces are recovered, but also that,  almost all of the SDAs have been identified as candidate traces.
% On the other hand, high threshold values result in the  opposite,yielding low recall and higher precision values.
% Notably, the word vector method achieved the highest precision value among the results when the threshold was set to 0.25.

% In conclusion, these results indicate that each method its strengths, and selecting the most suitable among them is dependent  on the specific project structure and requirements. \todo{I don't find this convinging argument. What kinds of structured would we prefer which methods for? Like what structure? It may depend more on how the project team documents its work as the traces are captured from the articulation of the team members and their code conventions (variable names, commenting etc.)}

We do not reach any conclusions based on our preliminary evaluation. This evaluation demonstrates that our approach is applicable for tracing requirements in a software repository but we refrain ourselves to be conclusive on the performance of the methods. In the near future we plan two additional evaluation studies.

The first study concerns evaluating our approach again in educational setting where we analyze the repositories of student groups, report on perceived usefulness and usability of our dashboard, study any correlations with the statistics reported by our dashboard and the performance of the groups in the course.

The second study is a case study with an industry partner where we ask for the ground truth from the experts from the industry and report the performance of different methods to extract trace links as well as perceived usefulness and usability of our dashboard in comparison to trace matrix, which is widely used in the industry for tracing requirements.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
