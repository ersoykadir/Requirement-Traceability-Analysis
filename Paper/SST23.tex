\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc} % To use Unicode (e.g. Turkish) characters
\usepackage[T1]{fontenc}
\renewcommand{\labelenumi}{(\roman{enumi})}
\usepackage{amsmath, amsthm, amssymb}
 % Some extra symbols
\usepackage[bottom]{footmisc}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{longtable}
\graphicspath{{figs/}} % Graphics will be here

\usepackage{multirow}
\usepackage{subcaption}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\usepackage{soul}
\usepackage[dvipsnames]{xcolor}
\DeclareRobustCommand{\fba}[1]{ {\begingroup\sethlcolor{BurntOrange}\hl{(fba:) #1}\endgroup} }
\DeclareRobustCommand{\sgk}[1]{ {\begingroup\sethlcolor{Yellow}\hl{(sgk:) #1}\endgroup} }
\begin{document}


\title{Title Comes Here }
\author{
 \IEEEauthorblockN{Kadir Ersoy, Ecenur Sezer, Suzan Üsküdarlı, Fatma Başak Aydemir}
\IEEEauthorblockA{
Boğaziçi University\\
Istanbul, Türkiye \\
\{kadir.ersoy, ecenur.sezer, suzan.uskudarli, basak.aydemir\}@boun.edu.tr}
}

\date{May 2021}

\maketitle
\begin{abstract}
    Requirements traceability refers to the capability of following the life of a requirement in both forwards and backward directions, and to the ability to link requirements to other software artifacts through particular relationships. These traceability links enable stakeholders to monitor the development progress of requirements and assist system engineers in tracking the effort and workload associated with fulfilling those requirements. Traditional methods to recover traceability links necessitate significant human effort, making them unsuitable and inefficient, particularly as the project scale grows. In this work, we present a tool that establishes automated requirement traceability links between requirements written in natural language and software artifacts acquired from GitHub repositories. The tool implements three optional methods to trace the artifacts to the requirements, namely a keyword extraction pipeline, TF-IDF vector and word-vector. The captured traces are stored in a graph database and visualized. Additionally, an interactive dashboard featuring statistical data about the artifacts and their traces is implemented. By offering automated traceability and comprehensive visualization, this tool aims to enhance the management of requirements and to understand their quality in software development projects.
\end{abstract}
\begin{IEEEkeywords}
Requirement traceability, NLP, Keyword extraction, Traceability graph
\end{IEEEkeywords}

\section{Introduction} \label{section:introduction}

% \begin{itemize}
%     \item Requirements traceability definition
%     \item Why do we do it, examples
%     \begin{itemize}
%         \item It is too hard to do it manually
%         \begin{itemize}
%             \item Cost of integration: Need trace link recovery, 
%             \item Human effort: Do not force extra measures for traceability
%         \end{itemize}
%         \item Its role in project management
%         \begin{itemize}
%             \item Monitoring the progress: Status of the project
%             \item Effort assessment: How much effort went to which features
%             \item Detecting problems for specific features
%         \end{itemize}
%     \end{itemize}
%     \item showcasing trace data and other stats to user in a self-evident manner
% \end{itemize}

Requirements traceability refers to the capability of following the life of a requirement in both forwards and backward directions\cite{gotel-1994}, and to the ability to link requirements to other software artifacts through particular relationships. Requirement traceability is crucial in software development projects since it shows the progress done in implementing the requirements and assists engineers in observing the effort and workload associated with them. All these aspects of requirement traceability help teams to ensure the fulfillment of specified requirements, by linking them to other software artifacts. Given that, it is a critical obstacle that traditional methods of recovering traceability links require significant time and human effort, making them unsuitable for the task as the project scale grows. Moreover, the quality of these methods is not consistent, since it highly depends on the understanding of the requirement traceability of the human analysts' performing them. 
In this paper, we present a tool implemented to automate the process of identifying trace links. Our tool establishes links from requirements that are written in natural language to the software development artifacts, mainly Issues, PRs, and Commits, that are fetched from GitHub repositories. By automatizing this process, our tool aims to decrease the reliance on human effort and provides efficient requirement traceability management.

For identifying the trace links, three optional methods are implemented in our tool. These methods are keyword extraction, which performs keyword matching to software development artifacts, and TF-IDF Vector - Word Vector methods, which basically perform vector-based similarity. Each of these methods offers distinct advantages that are highly dependent on the context of the projects.
The software artifacts and the identified trace links are stored in a graph database and visualized in a graphical structure to enable efficient retrieval of traceability information. To improve the comprehension of the collected traceability data, our tool also features an interactive dashboard. This dashboard is equipped with valuable statistical insights about the lifetime of the project and the requirements. Along with the visualization of the traceability links and the offering of statistical data, we aim to improve the management of the requirements, ultimately leading to better-planned and programmed software projects.

%\section{Problem and Motivation}
%\label{section:problem}

% Requirements traceability, 
%     - definition, 
% Why do we do it, examples
%     - It is too hard to do it manually
%     - Its role in project management,
%         - Monitoring the progress
%             - Status of the project
%             - What has been done 
%         - Effort assessment
%             - How much effort went to which features
%         - Detecting problems for specific features
%             - In the case of a problem, look for it in a localized manner 
%             - can be done since we know what has done for which context
%     - The use of requirements and req traceability is not common
%         - Cost of integration
%             - Need trace link recovery, 
%         - Human effort
%             - Do not force extra measures for traceability
%     - To achieve wide-use of traceability
%         - user friendly, 
%             - showcasing trace data and other stats to user in a self-evident manner
%         - minimal human effort, 


\section{Related Work}
\label{section:related_work}

There are several studies that focus on automatizing requirements traceability by implementing various techniques. Hayes et al. (2003)\cite{hayes-2003} have studied an approach that frames requirement traceability as an information retrieval(IR) problem. Their study aims to enhance recall and precision values of capturing traces. The IR methods they have implemented performed a significantly higher retrieval percentage, relative to the manual tracing. In a different study, Abdeen (2023)\cite{abdeen-2023} established a system that grips taxonomic trace links to link software development artifacts and requirements. In this approach, requirement artifacts are labeled with system-recommended labels based on their textual data. Bonner et al. (2023)\cite{bonner-2023} have developed a tool that performs Artificial Intelligence(AI) to identify trace links between requirements and model-based designs. They have integrated their tool into the Siemens toolchain for Application Lifecycle Management(ALM) and performed experiments on the traceability problem with the natural language requirements and system design models. Cleland-Huang et al. (2007)\cite{cleland-huang-2007} focused on nine best practices for implementing automated traceability. These practices include information retrieval methods, and they have significantly diminished the amount of effort given to produce a requirement trace matrix. Mills (2017)\cite{mills-2017} automated traceability link recovery(TLR) with machine learning algorithms and approached TLR as a binary classification problem.

These studies have integrated relatively more modern technologies such as artificial intelligence, machine learning, and information retrieval algorithms to automated trace links recovery and encouraged the researches to implement various approaches to minimize the effort and maximize the quality of requirement traceability.



\section{Method}
\label{section:method}
% \begin{itemize}
%     \item Inputs-outputs
%     \item Workflow + diagrams
%     \item Heuristics:
%     \begin{itemize}
%         \item Keyword extraction, more detailed than poster
%         \item Vector-based methods, more detailed than poster
%     \end{itemize}
%     \item Implementation:
%     \item \begin{itemize}
%         \item Trace graph + visuals
%         \item Dashboard
%         \item (Using same example for trace finding and trace graph)
%     \end{itemize}
% \end{itemize}

The tool expects a Requirement Specification Document(RSD) in the form of a text file written in natural language, along with a URL to the GitHub repository of the software project. The software development artifacts, namely Issues, PRs and Commits are fetched from the repository leveraging the GitHub API
and the requirement statements are parsed from the given RSD. This operation yields files containing information about Software Development Artifacts (SDA) and requirements, with their associated properties, such as title, description, creation and closure dates, status, and URLs. These artifacts serve as the data source for establishing trace links.

In the subsequent step, the file containing the information about SDAs and RDS are parsed to create graph nodes. At the completion of this operation, there are nodes representing each requirement and each SDA. The properties of the artifacts are stored as a property of the graph nodes. Later on, these nodes are saved into the graph database.

Furthermore, the trace links for each requirement statement are identified. The tool has three different methods implemented for trace link recovery. The first method involves a keyword extraction, where the significant keywords from requirement specification are extracted using a custom pipeline that utilizes \textit{dependency parsing}. These keywords serve as the basis for identifying trace links between SDAs and requirements. Keyword-matching is performed on the textual data of each SDA, to capture artifacts that include the keyword as a candidate trace. The second method utilizes the TF-IDF (Term Frequency-Inverse Document Frequency) algorithm, which creates a vector for each artifact, based on the importance of the terms in their textual data. By comparing the TF-IDF vectors, the potential trace links are identified. The third method acquires word embeddings for each artifact text using a pre-trained model, creating a document vector by averaging the word embeddings, and compares these vectors to identify traces, similar to the TF-IDF vector method. 

After the identification of the trace links, the graph database is utilized to store the trace links, along with requirements and SDAs. The use of a graph database offers a scalable solution and facilitates easy retrieval of traceability data through queries. 

Additionally, an interactive dashboard that visualizes the statistical information about the project and the requirements is developed. The dashboard includes various reports suitable for each kind of insight displayed, and facilitates a deeper understanding of the status and the complexity of the requirements, and provides information about the effort given to requirements during their lifetime.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.45\linewidth]{figs/toolflow.png}
    \caption{Control flow of the system.}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\linewidth]{figs/tracemethods.png}
    \caption{Control flow of the trace methods.}
    \label{fig:enter-label}
\end{figure}


\subsection{Heuristics}
\label{sec:heuristics} 
%\textit{Heuristic 1 - Substring Match}. Our first heuristic depends on the substring matches that can be observed among noun phrases. This process can be briefly explained as clustering noun phrases with the subset of noun phrases that contain a particular noun phrase as a substring. For example, "item label" and "item label size" noun phrases will be clustered under the "item label" key because "item label size" denotes a property of "item label". By applying this heuristic, all of these noun phrases are clustered into a set and the shortest noun phrase will represent these noun phrases. This heuristic reduces the size of the initial set of noun phrases. The reduction in the size of the initial set of noun phrases also decreases the time and space complexity of further steps of processing.

%After applying the first heuristic, we manage to cluster some of the model labels. However, there are still independent model labels. Even though the models are generated by modeling experts, modelers with different backgrounds may use different words for the same concept. For example, "shipment options" and "shipping offers" will be matched as similar noun phrases because of the high level of relatedness between the two of the noun phrases. To increase coverage of the NLP pipeline, we benefit from BERT \cite{devlin2018bert} which is a state-of-the-art deep language model. Thus, glossary extraction from models is provided by measuring the similarity of different noun phrases to each other and detecting different noun phrases that refer to a similar meaning via pre-trained sentence embeddings.

%First, noun phrases are encoded using sentence transformers which are provided by Hugging Face\footnote{https://huggingface.co} and are designed to be used with a pre-trained model. There is a variety of pre-trained sentence embedding models available for sentence embedding. We choose \textit{paraphrase-mpnet-base-v2} due to its high performance on STSbenchmark test. This sentence encoding process yields vectors that have 768 dimensions for each noun phrase. To extract similar noun phrases, pairwise cosine similarities are calculated between each noun phrase, and the symmetrical similarity matrix is obtained.




%\begin{algorithm}[htp]
%\SetAlgoLined
%\SetKwInOut{input}{Input}
%\SetKwInOut{output}{Output}
%\input{parent term P and term T}
%\output{parent term}
%parent = P\;
%term\_string = T + parent\;
%keywords = extracted keyword and probability pairs from term\_string using KEYBERT\\
%\uIf{length of keywords $>$ 1}{
%set most probable keyword as parent\\
%}
%\textbf{return} parent\;
% \caption{Heuristic 3}
%\end{algorithm}

%\textit{Heuristic 3 - Keyword Extraction}. Keyword extraction methods are mostly used for extracting the most representative n-grams from the texts. In this heuristic, lemmatized versions of both key and value are collected for each pair and concatenated into a single string. (Line 2) KeyBERT is tuned to extract unigrams from that concatenated string. (Line 3) If there exists a smaller common unit of representation for a key-value pair, both key and value are clustered under that new representation. (Lines 4 - 5)

%After applying these steps, all noun phrases extracted from the models are stored in a dictionary for reporting and visualized using an undirected graph for better understanding. Fig \ref{fig:graph} shows a sample representation extracted glossaries. Blue vertices denote matching model labels with concepts and red vertices (self-links) denote the mismatched model labels.

%\begin{figure}[htp]
%    \centering
%    \includegraphics[width=0.9\linewidth]{figs/graph.png}
%    \caption{Sample graph representation of extracted terms}
%    \label{fig:graph}
%\end{figure}

\subsection{Implementation}
\label{sec:impl}


\emph{Dashboard}
\\

In order to reinforce the visualization of traceability data, our research includes the development of an interactive dashboard, which offers various reports that display statistical insights. Each of these reports was carefully selected to effectively visualize the corresponding data, hence they have user-friendly designs for enhanced comprehension. Moreover, many of these reports are interactive and they allow users to trace the specific requirements, thereby enabling a more customized exploration of the traceability information.
The dashboard was implemented using Neodash technology, to provide full integrity with the traceability graph that is stored in the Neo4j database.

Fig. 3 represents a screenshot taken from the dashboard. The first report featured in the screenshot is a stacked bar chart showing the number of created Issues(in green) and PRs(in brown) for the BUcademy application. It could be observed that the highest number of created artifacts were in week 3 and week 7. The next report is another stacked bar chart that represents the number of artifacts opened and closed per week. Both of these charts are providing information about the status of development in the project lifetime and they help to identify periods of high or low development intensity and let users observe the assessment of the overall project dynamics.
The dashboard also provides static data regarding the number of open/closed issues, the number of open/merged PRs, and the average number of trace links per requirement. These statistics serve as a snapshot of the current state of the project.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/dashboard-barcharts.png}
    \caption{Bar charts and static data on dashboard.}
    \label{fig:dashboard}
\end{figure}

Fig. 4 continues with the line chart that illustrates the number of trace links for each requirement. By comparing the number of traces for each requirement, the report demonstrates the outliners, that could have relatively less or more traces. This comparison allows the users to observe the effort associated with each requirement since it displays the number of software development artifacts traced to them.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/linechart.png}
    \caption{Line chart on dashboard.}
    \label{fig:enter-label}
\end{figure}

Fig. 5 includes an example of the Sankey chart present in the dashboard that showcases the commonly traced artifacts between the chosen requirements while highlighting the strength of the trace links using the weight property. Thicker links represent a stronger relation from the requirement to the traced artifact. Thus, the chart represents the complementarity of the selected requirements.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/sankey.jpg}
    \caption{Sankey chart on dashboard.}
    \label{fig:enter-label}
\end{figure}

As displayed in Fig. 6, in addition to the generalized insights about the project status, various reports are implemented offering similar data for a specific requirement selected by the users. These include bar charts, similar to the earlier reports, tabular and graphical versions of the identified traces for the selected requirement. Overall, the current status and weekly effort given on a selected requirement are displayed in the last section of the dashboard.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/perreq.jpg}
    \caption{Insights for the selected requirement on dashboard.}
    \label{fig:enter-label}
\end{figure}


%We build a collaborative modeling environment by integrating two existing tools: \emph{i.} piStar \cite{pimentel2018pistar} for goal modeling and \emph{ii.} bpmn.io\footnote{https://bpmn.io} for business process modeling. We choose these two modeling environments because they are both open-source projects and the modeling languages they support are widely used for requirements engineering.

%\begin{figure}[htp]
%\centering
%  \includegraphics[width=\linewidth]{figs/arch.png}
%  \caption{Architecture of the system}
%  \label{fig:architecture}
%\label{label-c}
%\end{figure}

%We use two different open-source modeling tools for goal and business process modeling to form a viable combined modeling environment. When the users complete their work on a model, they will commit the model to the system. This committed work is used to update existing glossaries via the glossary extraction module of the system. The communication channel between the user and the system is provided by a web service. This web service is responsible for keeping track of the model versions to demonstrate changes in glossary over time, storing the model file in the disk, extracting model labels from the XML and JSON outputs of the goal and business process models, and transferring the extracted information to the database for glossary extraction from the models. Fig. \ref{fig:architecture} demonstrates the architecture of our system.

\section{Evaluation Plan}
\label{section:evaluation}

% \begin{itemize}
%     \item Ground truth 
%     \item Experiment setup
%     \item Results
%     \item Evaluation of results
% \end{itemize}

In this section, we present the evaluation methods we performed for our tool, focusing on the accuracy of the three implemented methods for identifying trace links: keyword extraction, TF-IDF vector and word vector.

For this purpose, we designed an experiment based on a ground truth set. The information present on the set was obtained from BUcademy\cite{bounswe-bucademy} open-source project. This set consisted of eight requirement statements and software development artifacts that trace to them. These trace links were captured manually by our team. The ground truth set served as a reference to assess the accuracy of the automated methods. 

The evaluation metrics used in this experiment were recall and precision values. Recall value presents the percentage of the real traces that are recovered, while precision presents the percentage of the recovered traces that are correct. These values are calculated using the formulas below:
\\

$Recall = \dfrac{True Positives}{True Positives + False Negatives}$
\\

$Precision = \dfrac{True Positives}{True Positives + False Positives}$
\\

In the experiment, true positives indicate the trace links that are identified by the method. False negatives indicate the trace links that are not identified(missed), and false positives indicate the identified trace links that are incorrect. The results of the keyword extraction method are presented in Table I, while the results of the vector based methods are presented in Table II.


\begin{table}[htb]
\centering
\vspace{1ex}
\begin{tabular}{l l l l}
\textbf{Method} & \textbf{Recall} & \textbf{Precision}\\

Keyword extraction & \textcolor{red}{0.865} & 0.212 \\

\end{tabular}
\caption{Recall - Precision Values of Keyword Extraction Method}
\end{table}

\begin{table}[htb]
\centering
\vspace{1ex}
\begin{tabular}{l l l l l}

\textbf  & \multicolumn{2}{c}{\textbf{Word-vector}} &  \multicolumn{2}{c}{\textbf{Tf-idf vector}}\\
\textbf{Sim.Threshold   } & \textbf{Rec.} &\textbf{Prec.} &  \textbf{Rec.} &\textbf{Prec.}\\

0.05    & 1.0 & 0.043 & 0.839 & 0.121 \\
0.15    & 1.0 & 0.043 & 0.573 & 0.256 \\
0.25    & 1.0 & 0.043 & 0.244 & 0.430 \\
0.35    & 1.0 & 0.043 & 0.095 & 0.392 \\
0.45    & 0.965 & 0.071 & 0.025 & 0.125 \\
0.55    & 0.865 & 0.100 & 0.013 & 0.121 \\
0.65    & 0.294 & 0.300 & 0 & 0 \\

\end{tabular}
\caption{Recall - Precision Values of Vector-based Methods}
\end{table}

Among the tested methods, the keyword extraction method yielded the best recall-precision pair. Conversely, the performance of the vector-based methods varied significantly by the defined threshold value. For the vector-based methods, the recall value was observed as close to 1 when the threshold value was relatively low, while the precision value dropped substantially, indicating that all the necessary traces are recovered but also almost all of the SDAs have been identified as candidate traces. On the other hand, setting the threshold value too high caused the opposite effect, resulting in low recall and higher precision values. 
% Notably, the word vector method achieved the highest precision value among the results when the threshold was set to 0.25. 

In conclusion, these results indicate that each method has strong aspects, and selecting the most suitable method highly depends on the specific project structure and requirements. 



\section{Conclusions}\label{section:conclusion}

% \begin{itemize}
%     \item Observations
%     \item Threads to validity
%     \item Future Work
% \end{itemize}

In this paper, we presented a tool that automates the traceability link recovery by identifying trace links between requirements written in natural language and software development artifacts(Issues, PRs, Commits) obtained from GitHub repositories. Through the implementation of three optional methods, namely keyword extraction, TF-IDF vector and word vector, we have demonstrated the capability of identifying traceability links of our tool. We have stored the traceability data we obtained in a graph database to visualize the trace links, and supported the data with an interactive dashboard, which provides users with comprehensive statistical insights.
Our experiments on the evaluation of the tool have led us to observe that the quality and consistency of the requirement statements and the software development artifacts(SDA) significantly impact the effectiveness of the tool's capability of identifying trace links. Well-written requirement specifications that are self-explanatory and granulated lead to easier identification of its traces along with SDA which have textual data (e.g. title, description, comments) that is semantically related to the feature they implement. In this manner, our tool could be utilized for educational purposes, to assist in the improvement of writing requirement specifications.

It is important to acknowledge the threats to the validity of our study. Firstly, the absence of a ground truth set that is established by analysts may introduce bias in our evaluation process. We have prepared the ground truth set and conducted our experiments on the student projects we have participated in. Thus, we do know the capabilities of our tool and the project that we experimented on. Future work should involve validation with industry professionals and experiments designed with a wider range of projects to enhance the operating scale of our tool. 

Other several avenues for future work include providing trace links between different kinds of software artifacts, such as from Issues to PRs, to enable a deeper view of the development process. Additionally, we plan to integrate different types of requirement specifications, to accommodate diverse requirement specification document formats. Furthermore, we intend to develop a product that fetches software artifacts from various platforms such as GitLab and Jira, to improve the compatibility of the tool with different development environments that are mostly preferred within the industry. Finally, we would like to expand the variety of methods used in identifying trace links based on the needs of the users and improve the accuracy.
% by increasing the recall and precision values obtained.

In conclusion, our tool provides an automated solution for requirement traceability(RT) and enhances the management of software projects. By optimizing the time and effort required for RT and offering educational opportunities for project management, our tool contributes to the overall success of software development projects. With future development and evaluation, our study carries the potential to positively impact traceability practices.



%We present a shared collaborative modeling environment instrumented with an automated glossary extraction pipeline from model labels using state-of-the-art NLP techniques without any supervision. Our approach prevents stakeholders from spending effort on a challenging and time-consuming task of manual glossary extraction. We develop two heuristics that use pre-trained BERT models and one simple heuristic that depends on substring matching.

 %In this section, we share our plans for glossary extraction. First, we plan to conduct an experimental evaluation with human experts as we stated in Sect. \ref{section:evaluation}. Second, we plan to improve our glossary extraction heuristics' performance using human experts' feedbacks. Finally, we aim to extend our method for automated domain model extraction from model labels and employing our method as a complementary for existing systems to form a hybrid system.

\bibliographystyle{IEEEtran}
\bibliography{paper}
\end{document}